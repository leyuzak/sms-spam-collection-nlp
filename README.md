# ğŸ“© SMS Spam Detection using NLP

ğŸ”— **Live Demo (Hugging Face Spaces):**  
https://huggingface.co/spaces/leyuzak/SMS-Spam-Collection-using-NLP

ğŸ”— **Kaggle Notebook:**  
https://www.kaggle.com/code/leyuzakoksoken/sms-spam-collection-using-nlp

---

## ğŸ“Œ Project Overview

This project is an end-to-end **Natural Language Processing (NLP)** application designed to classify SMS messages as **Spam** or **Ham (Non-Spam)**.

The workflow covers the complete machine learning lifecycle, including:
- Exploratory Data Analysis (EDA)
- Text preprocessing
- Feature extraction
- Model training and interpretation
- Deployment as an interactive web application

---

## ğŸ¯ Objective

The main objective is to automatically identify spam SMS messages based solely on their text content.  
Spam messages typically contain promotional, urgent, or deceptive language, while ham messages are conversational and personal.

---

## ğŸ“Š Dataset

- **Name:** SMS Spam Collection Dataset  
- **Classes:**
  - `ham` â€“ legitimate messages
  - `spam` â€“ unsolicited or fraudulent messages
- **Characteristics:**
  - Highly imbalanced class distribution
  - Spam messages are generally longer
  - Distinct vocabulary patterns between classes

---

## ğŸ” Exploratory Data Analysis (EDA)

Key insights discovered during EDA:

- Strong class imbalance favoring ham messages
- Spam messages tend to have significantly greater text length
- Frequent spam keywords include *free*, *call*, *claim*, *win*
- Ham messages contain conversational words such as *ok*, *sorry*, *home*

These insights informed preprocessing and modeling decisions.

---

## ğŸ§¹ Data Preprocessing

The following preprocessing steps were applied to the text data:

- Conversion to lowercase
- Removal of URLs, emails, numbers, and punctuation
- Whitespace normalization
- Generation of a cleaned text feature for modeling

---

## ğŸ§  Feature Engineering & Modeling

- **Text Representation:** TF-IDF Vectorization  
  - Unigrams and bigrams  
- **Classifier:** Logistic Regression  
- **Imbalance Handling:** Class weighting (`class_weight="balanced"`)

This approach provides:
- Strong baseline performance
- Fast training and inference
- High interpretability via model coefficients

---

## ğŸ“ˆ Model Interpretation

The trained model highlights meaningful linguistic patterns:

### Top Spam Indicators
- `call`, `free`, `claim`, `win`, `reply`, `mobile`

### Top Ham Indicators
- `ok`, `sorry`, `home`, `later`, `love`, `got`

These results align with real-world characteristics of spam and non-spam messages.

---

## ğŸ–¼ï¸ Visualization

- Class distribution plots
- Text length distribution (ham vs spam)
- WordCloud visualizations for both classes
- Confusion matrix for model evaluation

---

## ğŸš€ Web Application

The trained model is deployed as a **Streamlit** application on **Hugging Face Spaces**.

### Features:
- ğŸ” Single message classification
- ğŸ“Š Spam probability score
- ğŸ“„ Batch prediction (multiple messages)
- ğŸ§¹ Optional display of cleaned text

Access the live demo here:  
ğŸ‘‰ https://huggingface.co/spaces/leyuzak/SMS-Spam-Collection-using-NLP

---

## ğŸ› ï¸ Tech Stack

- Python
- Scikit-learn
- TF-IDF
- Logistic Regression
- Streamlit
- Docker
- Hugging Face Spaces
- Kaggle

---

## ğŸ“Œ Results

The model achieves strong performance, particularly in detecting spam messages when evaluated using precision, recall, and F1-score.

The results demonstrate that classical NLP techniques remain effective for text classification tasks.

---

## ğŸ”® Future Improvements

Potential enhancements include:
- Transformer-based models (e.g., BERT)
- Threshold tuning for improved spam recall
- REST API deployment (FastAPI)
- Multi-language SMS support











import os
import re
import json
import string

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report)

import joblib


data_path = "/kaggle/input/sms-spam-collection-dataset/spam.csv"





df = pd.read_csv(data_path, encoding="latin-1")





df.head()


df.tail()


df.shape


df.describe()


df.info()





df = df.rename(columns={"v1": "label", "v2": "text"})


drop_cols = [c for c in df.columns if c not in ["label", "text"]]


df = df.drop(columns=drop_cols)


df["label"] = df["label"].astype(str).str.strip().str.lower()


df["text"] = df["text"].astype(str)


df["text_len"] = df["text"].str.len()


df["word_count"] = df["text"].apply(lambda x: len(x.split()))


df.describe(include="all")


counts = df["label"].value_counts()

plt.figure(figsize=(5, 3))
plt.bar(counts.index, counts.values, color="green")
plt.title("label distribution")
plt.xlabel("label")
plt.ylabel("count")
plt.show()


plt.figure(figsize=(7, 4))

for lab in ["ham", "spam"]:
    vals = df.loc[df["label"] == lab, "text_len"].values
    plt.hist(vals, bins=40, alpha=0.6, label=lab)

plt.title("text length distribution")
plt.xlabel("text_len")
plt.ylabel("frequency")
plt.legend()
plt.show()


def clean_text(x: str) -> str:
    x = x.lower()
    x = re.sub(r"http\S+|www\.\S+", " ", x)              
    x = re.sub(r"\S+@\S+", " ", x)                        
    x = re.sub(r"\d+", " ", x)                            
    x = x.translate(str.maketrans("", "", string.punctuation))
    x = re.sub(r"\s+", " ", x).strip()
    return x


df["x"] = df["text"].apply(clean_text)


label2id = {"ham": 0, "spam": 1}


id2label = {0: "ham", 1: "spam"}


df["y"] = df["label"].map(label2id).astype(int)


df[["label", "text", "x", "y"]].head()





x = df["x"].values


y = df["y"].values


x_train, x_test, y_train, y_test = train_test_split(
    x, y,
    test_size=0.2,
    random_state=42,
    stratify=y)


vectorizer = TfidfVectorizer(
    ngram_range=(1, 2),
    min_df=2,
    max_df=0.95)


x_train_vec = vectorizer.fit_transform(x_train)


x_test_vec  = vectorizer.transform(x_test)


model = LogisticRegression(
    max_iter=2000,
    class_weight="balanced")


model.fit(x_train_vec, y_train)


pred = model.predict(x_test_vec)


metrics = {
    "accuracy": float(accuracy_score(y_test, pred)),
    "precision_spam": float(precision_score(y_test, pred, pos_label=1)),
    "recall_spam": float(recall_score(y_test, pred, pos_label=1)),
    "f1_spam": float(f1_score(y_test, pred, pos_label=1)),}
metrics


cm = confusion_matrix(y_test, pred)

plt.figure(figsize=(4, 4))
plt.imshow(cm)
plt.title("confusion matrix")
plt.xticks([0, 1], ["ham", "spam"])
plt.yticks([0, 1], ["ham", "spam"])
plt.xlabel("pred")
plt.ylabel("true")

for (i, j), v in np.ndenumerate(cm):
    plt.text(j, i, str(v), ha="center", va="center")

plt.show()





feature_names = np.array(vectorizer.get_feature_names_out())
coefs = model.coef_[0]

top_n = 20

top_spam_idx = np.argsort(coefs)[-top_n:][::-1]
top_ham_idx  = np.argsort(coefs)[:top_n]

top_spam = list(zip(feature_names[top_spam_idx], coefs[top_spam_idx]))
top_ham  = list(zip(feature_names[top_ham_idx], coefs[top_ham_idx]))


print("top spam indicators:")
for w, c in top_spam:
    print(f"{w:25s} {c:.4f}")


print("\n\ntop ham indicators:")
for w, c in top_ham:
    print(f"{w:25s} {c:.4f}")





from wordcloud import WordCloud

def make_wordcloud(text, title):
    wc = WordCloud(
        width=900,
        height=450,
        background_color="white",
        collocations=False  
    ).generate(text)

    plt.figure(figsize=(10, 4))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.title(title)
    plt.show()

spam_text = " ".join(df.loc[df["label"] == "spam", "x"].tolist())
ham_text  = " ".join(df.loc[df["label"] == "ham",  "x"].tolist())

make_wordcloud(spam_text, "wordcloud - spam")
make_wordcloud(ham_text, "wordcloud - ham")


out_dir = "sms_spam_tfidf_logreg"
os.makedirs(out_dir, exist_ok=True)

joblib.dump(model, os.path.join(out_dir, "model.joblib"))
joblib.dump(vectorizer, os.path.join(out_dir, "vectorizer.joblib"))

with open(os.path.join(out_dir, "label2id.json"), "w", encoding="utf-8") as f:
    json.dump(label2id, f, ensure_ascii=False, indent=2)

with open(os.path.join(out_dir, "metrics.json"), "w", encoding="utf-8") as f:
    json.dump(metrics, f, ensure_ascii=False, indent=2)

out_dir, os.listdir(out_dir)
